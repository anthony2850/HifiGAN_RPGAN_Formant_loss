{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+tbL0vZFUrKR3xqP968yw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthony2850/HifiGAN_RPGAN_Formant_loss/blob/main/hifigan_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzCT-GVno6Wj",
        "outputId": "29f1ebc7-1608-4afe-80b8-2cfc4ce358ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wavenet_vocoder\n",
        "!pip install librosa==0.9.1\n",
        "!pip install wandb\n",
        "!pip install \"pip<24.1\"\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "s1B5_nV8pBUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/paper_submit/PAPER_SUBMIT\n",
        "# 본인 폴더 경로로 변경"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOxohs45pEkJ",
        "outputId": "b4a63bdf-bf3c-440f-dfcf-7a8f658090ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1S6Y0Fb8MYqMax1o_dZBhgLlN3NSMxMXi/PAPER_SUBMIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "import easydict\n",
        "from torch.backends import cudnn\n",
        "import time\n",
        "import datetime\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.spatial.distance import euclidean\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "v5h_s0TMpFjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utils\n",
        "import glob\n",
        "import os\n",
        "import matplotlib\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "def plot_spectrogram(spectrogram):\n",
        "    fig, ax = plt.subplots(figsize=(10, 2))\n",
        "    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n",
        "                   interpolation='none')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    plt.close()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "\n",
        "def apply_weight_norm(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        weight_norm(m)\n",
        "\n",
        "\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def save_checkpoint(filepath, obj):\n",
        "    print(\"Saving checkpoint to {}\".format(filepath))\n",
        "    torch.save(obj, filepath)\n",
        "    print(\"Complete.\")\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '????????')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return None\n",
        "    return sorted(cp_list)[-1]"
      ],
      "metadata": {
        "id": "Izx_2K54pIfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#env\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def build_env(config, config_name, path): # env_path : cp_hifigan/config_v1.json에 저장\n",
        "    t_path = os.path.join(path, config_name)\n",
        "    if config != t_path:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        shutil.copyfile(config, os.path.join(path, config_name))"
      ],
      "metadata": {
        "id": "XNFznIn-pJ4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from librosa.util import normalize\n",
        "from scipy.io.wavfile import read\n",
        "from librosa.filters import mel\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    return np.exp(x) / C\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "mel_basis = {}\n",
        "hann_window = {}\n",
        "\n",
        "def librosa_mel_fn_(sampling_rate, n_fft, num_mels, fmin, fmax):\n",
        "    return librosa.filters.mel(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "\n",
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn_(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    # Apply padding and calculate STFT\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "\n",
        "    # STFT의 결과에서 절대값을 취해 크기만 남김\n",
        "    spec = spec.abs()\n",
        "\n",
        "    # spec의 차원이 (n_frames, frequency_bins) 형식으로 맞는지 확인\n",
        "    if len(spec.shape) == 3:\n",
        "        spec = spec.squeeze(1)\n",
        "\n",
        "    # mel_basis와 spec의 차원이 맞는지 확인한 후 matmul 실행\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n",
        "\n",
        "def get_dataset_filelist(a):\n",
        "    with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n",
        "\n",
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 device=None, fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split\n",
        "        self.n_fft = n_fft\n",
        "        self.num_mels = num_mels\n",
        "        self.hop_size = hop_size\n",
        "        self.win_size = win_size\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "        self.fmax_loss = fmax_loss\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.device = device\n",
        "        self.fine_tuning = fine_tuning\n",
        "        self.base_mels_path = base_mels_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename)\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning:\n",
        "                audio = normalize(audio) * 0.95\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate:\n",
        "                raise ValueError(f\"{sampling_rate} SR doesn't match target {self.sampling_rate} SR\")\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio).unsqueeze(0)\n",
        "\n",
        "        if not self.fine_tuning:  # self.fine_tuning : False\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "        else:\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "\n",
        "            if len(mel.shape) < 3:\n",
        "                mel = mel.unsqueeze(0)\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n",
        "\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ],
      "metadata": {
        "id": "baitZ9yQpLk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "\n",
        "LRELU_SLOPE = 0.1\n",
        "\n",
        "\n",
        "class ResBlock1(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock1, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs1:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.convs2:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class ResBlock2(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1])))\n",
        "        ])\n",
        "        self.convs.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c in self.convs:\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module): # inference 때와 TNSE때 feature vector 반환 설정\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
        "                                k, u, padding=(k-u)//2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = h.upsample_initial_channel//(2**(i+1))\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(h, ch, k, d))\n",
        "\n",
        "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_pre(x)\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x)\n",
        "        feature_vector=x # conv를 거치기 전의 feature vector를 확보\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x, feature_vector # TNSE때만 feature vector 반환\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        remove_weight_norm(self.conv_post)\n",
        "\n",
        "\n",
        "class DiscriminatorP(torch.nn.Module):\n",
        "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
        "        super(DiscriminatorP, self).__init__()\n",
        "        self.period = period\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        # 1d to 2d\n",
        "        b, c, t = x.shape\n",
        "        if t % self.period != 0: # pad first\n",
        "            n_pad = self.period - (t % self.period)\n",
        "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
        "            t = t + n_pad\n",
        "        x = x.view(b, c, t // self.period, self.period)\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiPeriodDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiPeriodDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorP(2),\n",
        "            DiscriminatorP(3),\n",
        "            DiscriminatorP(5),\n",
        "            DiscriminatorP(7),\n",
        "            DiscriminatorP(11),\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "class DiscriminatorS(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(DiscriminatorS, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n",
        "            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
        "            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
        "            # 히든 레이어 추가 실험\n",
        "\n",
        "            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiScaleDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorS(use_spectral_norm=True),\n",
        "            DiscriminatorS(),\n",
        "            DiscriminatorS(),\n",
        "        ])\n",
        "        self.meanpools = nn.ModuleList([\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            if i != 0:\n",
        "                y = self.meanpools[i-1](y)\n",
        "                y_hat = self.meanpools[i-1](y_hat)\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n"
      ],
      "metadata": {
        "id": "Vo_EI3BbpNH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RPGAN Loss"
      ],
      "metadata": {
        "id": "qvTECIS0W0o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            loss += torch.mean(torch.abs(rl - gl))\n",
        "\n",
        "    return loss*2\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs): #모양 바꿈\n",
        "     loss = 0\n",
        "     for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "         relativistic_logits = dr - dg\n",
        "         loss += torch.mean(F.softplus(-relativistic_logits))\n",
        "     return loss\n",
        "\n",
        "def generator_loss(disc_real_outputs, disc_generated_outputs): #discriminator가 뱉은 값을 받아서 업뎃을 해야지\n",
        "     loss = 0\n",
        "     for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "         relativistic_logits = dr - dg\n",
        "         loss += torch.mean(F.softplus(relativistic_logits))\n",
        "     return loss"
      ],
      "metadata": {
        "id": "S8gcs3vgpQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSGAN Loss"
      ],
      "metadata": {
        "id": "cVJrUdVPW2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            loss += torch.mean(torch.abs(rl - gl))\n",
        "\n",
        "    return loss*2\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ],
      "metadata": {
        "id": "VM7mwKLzW4Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.transforms import Resample\n",
        "import tempfile\n",
        "import librosa\n",
        "\n",
        "def lsd(est ,target):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim = 1) ** 0.5\n",
        "    lsd = torch.mean(squared_diff, dim = 0)\n",
        "    return lsd\n",
        "\n",
        "def lsd_hf(est, target, hf_ratio=0.25):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "\n",
        "    # Define high-frequency range\n",
        "    num_freq_bins = est.shape[0]\n",
        "    hf_start = int(num_freq_bins * (1 - hf_ratio))  # Starting index for high frequencies\n",
        "\n",
        "    # Focus on high-frequency bands\n",
        "    est_hf = est[hf_start:, :]\n",
        "    target_hf = target[hf_start:, :]\n",
        "\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target_hf + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est_hf + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim=1) ** 0.5\n",
        "    lsd_hf = torch.mean(squared_diff, dim=0)\n",
        "\n",
        "    return lsd_hf\n",
        "\n",
        "def extract_f0_from_audio(audio, sr, fmin=50, fmax=500):\n",
        "    audio_np = audio.cpu().numpy()  # Convert to numpy for librosa\n",
        "    f0, voiced_flag, _ = librosa.pyin(audio_np, fmin=fmin, fmax=fmax, sr=sr)\n",
        "    f0 = torch.tensor(f0, dtype=torch.float32)  # Convert back to tensor\n",
        "    f0[~torch.tensor(voiced_flag, dtype=torch.bool)] = 0  # Set unvoiced regions to 0\n",
        "    return f0\n",
        "\n",
        "def f0_rmse(f0_pred, f0_target):\n",
        "    assert f0_pred.shape == f0_target.shape, \"F0 shapes must match.\"\n",
        "    squared_error = (f0_pred - f0_target) ** 2\n",
        "    mse = torch.mean(squared_error)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "jkdxJCfEpTIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "DnEoNxsStCOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RPGAN+R1R2로 inference하고싶을 때"
      ],
      "metadata": {
        "id": "2rnaHhYzBCWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    group_name = None\n",
        "    input_wavs_dir = 'LJSpeech-1.1/wavs'\n",
        "    output_dirs = 'inference_output_rpgan_all'\n",
        "    checkpoint_file = 'cp_RPGAN_R1R2_lambda(0.0005)/g_00081810.pt'\n",
        "    input_validation_file = 'LJSpeech-1.1/validation.txt'\n",
        "    feature_output_dir = 'feature_output_rpgan'"
      ],
      "metadata": {
        "id": "UlMbzs_StBo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSGAN으로 inference하고싶을 때"
      ],
      "metadata": {
        "id": "0jez8zEzBHN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    group_name = None\n",
        "    input_wavs_dir = 'LJSpeech-1.1/wavs'\n",
        "    output_dirs = 'inference_output_lsgan_all'\n",
        "    checkpoint_file = 'cp_HiFi_GAN/g_00082018.pt'\n",
        "    input_validation_file = 'LJSpeech-1.1/validation.txt'\n",
        "    feature_output_dir = 'feature_output_lsgan'"
      ],
      "metadata": {
        "id": "xRuhIeLTYOgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "# from env import AttrDict\n",
        "# from meldataset import mel_spectrogram, MAX_WAV_VALUE, load_wav\n",
        "# from models import Generator\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "  return mel_spectrogram(x, h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size, h.fmin, h.fmax)\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def get_inference_dataset(a, num_samples=150,start_idx=0): #여기서 num_sample로 개수 조절\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    return validation_files[start_idx:num_samples+start_idx]  # 여기서 1-50/ 51-100/ 101-150 조절\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = get_inference_dataset(a,num_samples=150)\n",
        "\n",
        "    os.makedirs(a.output_dirs, exist_ok=True)\n",
        "\n",
        "    os.makedirs(a.feature_output_dir,exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file_path in filelist:\n",
        "            wav, sr = load_wav(file_path)\n",
        "            wav = wav / MAX_WAV_VALUE\n",
        "            wav = torch.FloatTensor(wav).to(device)\n",
        "            x = get_mel(wav.unsqueeze(0))\n",
        "\n",
        "            y_g_hat,_ = generator(x)\n",
        "\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            output_filename = os.path.splitext(os.path.basename(file_path))[0] + '_lsgan_inference.wav'\n",
        "            output_file = os.path.join(a.output_dirs, output_filename)\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--input_wavs_dir', default='test_files')\n",
        "    # parser.add_argument('--output_dirs', default='generated_files')\n",
        "    # parser.add_argument('--checkpoint_file', required=True)\n",
        "    # a = parser.parse_args()\n",
        "\n",
        "    config_file = \"config_v1.json\"\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a = Args()\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LS8doJIctHbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator 마지막 레이어 거치기 전 Feature Map 추출"
      ],
      "metadata": {
        "id": "H04WRVV3P7Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "# from env import AttrDict\n",
        "# from meldataset import mel_spectrogram, MAX_WAV_VALUE, load_wav\n",
        "# from models import Generator\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "  return mel_spectrogram(x, h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size, h.fmin, h.fmax)\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def get_inference_dataset(a, num_samples=150,start_idx=0): #여기서 num_sample로 개수 조절\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    return validation_files[start_idx:num_samples+start_idx]\n",
        "\n",
        "'''\n",
        "def feature_extraction(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = get_inference_dataset(a,num_samples=150)\n",
        "\n",
        "    os.makedirs(a.feature_output_dir,exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "\n",
        "    feature_maps = []\n",
        "    original_lengths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file_path in filelist:\n",
        "            wav, sr = load_wav(file_path)\n",
        "            wav = wav / MAX_WAV_VALUE\n",
        "            wav = torch.FloatTensor(wav).to(device)\n",
        "            x = get_mel(wav.unsqueeze(0))\n",
        "\n",
        "            _,feature_map = generator(x)\n",
        "            feature_np=feature_map.detach().cpu().numpy()\n",
        "            feature_maps.append(feature_np)\n",
        "\n",
        "            original_lengths.append(feature_np.shape[-1])\n",
        "\n",
        "    feature_save_path = os.path.join(a.feature_output_dir, \"feature_maps.npy\")\n",
        "    np.save(feature_save_path, np.array(feature_maps,dtype=object), allow_pickle=True)\n",
        "\n",
        "    length_save_path = os.path.join(a.feature_output_dir,\"lengths.npy\")\n",
        "    np.save(length_save_path, np.array(original_lengths))\n",
        "\n",
        "    print(f\"Feature maps saved to {feature_save_path}\")\n",
        "    print(f\"Original lengths saved to {length_save_path}\")\n",
        "'''\n",
        "def feature_extraction(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = get_inference_dataset(a, num_samples=150)\n",
        "\n",
        "    os.makedirs(a.feature_output_dir, exist_ok=True)  # Feature 저장 폴더 생성\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "\n",
        "    original_lengths = []  # 원본 길이 저장\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, file_path in enumerate(filelist):\n",
        "            wav, sr = load_wav(file_path)\n",
        "            wav = wav / MAX_WAV_VALUE\n",
        "            wav = torch.FloatTensor(wav).to(device)\n",
        "            x = get_mel(wav.unsqueeze(0))\n",
        "\n",
        "            _, feature_map = generator(x)\n",
        "            feature_np = feature_map.detach().cpu().numpy()\n",
        "\n",
        "            # 🔥 개별 Feature Map 저장\n",
        "            feature_filename = f\"feature_{idx}.npy\"\n",
        "            feature_path = os.path.join(a.feature_output_dir, feature_filename)\n",
        "            np.save(feature_path, feature_np)\n",
        "\n",
        "    print(f\"✅ 개별 Feature Maps 저장 완료! 경로: {a.feature_output_dir}\")\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--input_wavs_dir', default='test_files')\n",
        "    # parser.add_argument('--output_dirs', default='generated_files')\n",
        "    # parser.add_argument('--checkpoint_file', required=True)\n",
        "    # a = parser.parse_args()\n",
        "\n",
        "    config_file = \"config_v1.json\"\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    feature_extraction(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a = Args()\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9zn4OhhP-PE",
        "outputId": "23213feb-0862-4f75-87f4-b3f0d7e6ffe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Inference Process..\n",
            "Loading 'cp_HiFi_GAN/g_00082018.pt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-32dc0e7966e0>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(filepath, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete.\n",
            "Removing weight norm...\n",
            "✅ 개별 Feature Maps 저장 완료! 경로: feature_output_lsgan\n",
            "✅ Original lengths saved to feature_output_lsgan/lengths.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def prepare_inference_dataset(a, num_samples=150, start_idx=0):\n",
        "    \"\"\"\n",
        "    input_wavs_dir에서 지정된 개수(num_samples)의 WAV 파일을 가져와\n",
        "    inference_input 폴더에 저장하는 함수\n",
        "    \"\"\"\n",
        "    source_dir = a.input_wavs_dir  # 원본 WAV 파일들이 있는 폴더\n",
        "    target_dir = \"inference_input\"  # WAV 파일을 저장할 폴더\n",
        "\n",
        "    # 폴더가 없으면 생성\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # 원본 validation 파일 목록 가져오기\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(source_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    # 선택된 범위만큼 파일 가져오기\n",
        "    selected_files = validation_files[start_idx:num_samples+start_idx]\n",
        "\n",
        "    # 파일 복사\n",
        "    copied_files = []\n",
        "    for file_path in selected_files:\n",
        "        if os.path.exists(file_path):\n",
        "            target_path = os.path.join(target_dir, os.path.basename(file_path))\n",
        "            shutil.copy(file_path, target_path)  # 파일 복사\n",
        "            copied_files.append(target_path)\n",
        "        else:\n",
        "            print(f\"Warning: File not found - {file_path}\")\n",
        "\n",
        "    print(f\"✅ {len(copied_files)} WAV files copied to '{target_dir}'\")\n",
        "\n",
        "    return copied_files  # 복사된 파일 리스트 반환"
      ],
      "metadata": {
        "id": "mMaShz96t9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wav2Vec2 Feature 추출"
      ],
      "metadata": {
        "id": "IlzsA1LaGMWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Real dataset의 wav2vec2.0 feature 추출\n",
        "import torchaudio.transforms as T\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large\").to(device).eval()\n",
        "\n",
        "os.makedirs('feature_output_wav2vec2_rpgan_0303', exist_ok=True)\n",
        "\n",
        "with open(Args.input_validation_file, 'r', encoding='utf-8') as f:\n",
        "  file_list = [line.split('|')[0] for line in f.read().split('\\n') if len(line)>0]\n",
        "\n",
        "wav_files = [os.path.join(Args.input_wavs_dir, f\"{file}.wav\") for file in file_list]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, file_path in enumerate(wav_files):\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"⚠️ Skipping: {file_path} (File Not Found)\")\n",
        "            continue\n",
        "\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "\n",
        "        # 샘플링 레이트 변환 (16kHz 필요)\n",
        "        if sr != 16000:\n",
        "            transform = T.Resample(orig_freq=sr, new_freq=16000)\n",
        "            waveform = transform(waveform)\n",
        "\n",
        "        input_values = waveform.to(device)\n",
        "\n",
        "        # ✅ Wav2Vec2를 이용한 feature 추출\n",
        "        outputs = model(input_values)\n",
        "        feature_map = outputs.last_hidden_state  # 🔥 최종 출력 feature (T, 1024)\n",
        "\n",
        "        # ✅ Time Mean Pooling 적용 → (1024,)\n",
        "        feature_mean = feature_map.mean(dim=1).cpu().numpy().squeeze(0)\n",
        "\n",
        "        # ✅ Feature 저장\n",
        "        feature_save_path = os.path.join('feature_output_wav2vec2_real', f\"feature_{idx}.npy\")\n",
        "        np.save(feature_save_path, feature_mean)\n",
        "\n",
        "        print(f\"✅ Saved: {feature_save_path} - Shape: {feature_mean.shape}\")"
      ],
      "metadata": {
        "id": "w2AYVq8qM7HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fake dataset의 wav2vec2.0 feature 추출\n",
        "import torchaudio.transforms as T\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large\").to(device).eval()\n",
        "\n",
        "wav_dir = 'inference_output_lsgan_all' #or 'inference_output_lsgan_all'\n",
        "feature_output_dir = 'feature_output_wav2vec2_lsgan' #or 'feature_output_wav2vec2_lsgan'\n",
        "os.makedirs(feature_output_dir, exist_ok=True)\n",
        "\n",
        "wav_files = sorted(glob.glob(f\"{wav_dir}/*.wav\"))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx, file_path in enumerate(wav_files):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "\n",
        "    if sr!= 16000:\n",
        "      transform = T.Resample(orig_freq=sr, new_freq=16000)\n",
        "      waveform = transform(waveform)\n",
        "\n",
        "    input_values = waveform.to(device)\n",
        "\n",
        "    outputs = model(input_values, output_hidden_states=True)\n",
        "    feature_map = outputs.last_hidden_state\n",
        "\n",
        "    feature_mean = feature_map.mean(dim=1).cpu().numpy().squeeze(0)\n",
        "\n",
        "    feature_save_path = os.path.join(feature_output_dir, f\"feature_{idx}.npy\")\n",
        "    np.save(feature_save_path, feature_mean)\n",
        "\n",
        "    print(f\"✅ Saved: {feature_save_path} - Shape: {feature_mean.shape}\")\n"
      ],
      "metadata": {
        "id": "yzBZ_tu2GRqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wasserstein Distance 측정 (Wav)"
      ],
      "metadata": {
        "id": "-K2h9E7zurlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "def load_wav(file_path):\n",
        "  waveform, sample_rate = torchaudio.load(file_path)\n",
        "  return waveform.squeeze(0).numpy(), sample_rate\n",
        "\n",
        "def compute_wd(wav_file_1, wav_file_2):\n",
        "  wd_values =[]\n",
        "  for path_1, path_2 in zip(wav_file_1, wav_file_2):\n",
        "    if not os.path.exists(path_1) or not os.path.exists(path_2):\n",
        "      print(f\"Warning: File not found - {path_1} or {path_2}\")\n",
        "      continue\n",
        "\n",
        "    wav_1, _ = load_wav(path_1)\n",
        "    wav_2, _ = load_wav(path_2)\n",
        "\n",
        "    min_length = min(len(wav_1), len(wav_2))\n",
        "    if min_length == 0:\n",
        "      print(f\"Warning: Empty WAV file - {path_1} or {path_2}\")\n",
        "      continue\n",
        "\n",
        "    wav_1, wav_2 = wav_1[:min_length], wav_2[:min_length]\n",
        "\n",
        "    #WD 계산을 위한 float 변환\n",
        "    wav_1 = wav_1.astype(np.float32)\n",
        "    wav_2 = wav_2.astype(np.float32)\n",
        "\n",
        "    wd = wasserstein_distance(wav_1, wav_2)\n",
        "    wd_values.append(wd)\n",
        "\n",
        "  return np.mean(wd_values) if len(wd_values) > 0 else float('nan')\n",
        "\n",
        "\n",
        "def get_wav_files(folder_path):\n",
        "    \"\"\"지정된 폴더에서 모든 WAV 파일 리스트 반환\"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder not found - {folder_path}\")\n",
        "        return []\n",
        "\n",
        "    wav_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
        "\n",
        "    if not wav_files:\n",
        "        print(f\"Warning: No WAV files found in {folder_path}.\")\n",
        "\n",
        "    return wav_files\n",
        "\n",
        "# Example usage:\n",
        "folder_1 = \"inference_input\"\n",
        "folder_2 = \"inference_output_rpgan_0303\"\n",
        "\n",
        "wav_files_1 = get_wav_files(folder_1)\n",
        "wav_files_2 = get_wav_files(folder_2)\n",
        "\n",
        "if len(wav_files_1) == len(wav_files_2) and len(wav_files_1) > 0:\n",
        "    avg_wd = compute_wd(wav_files_1, wav_files_2)\n",
        "    print(f\"Average Wasserstein Distance: {avg_wd:.10f}\")\n",
        "else:\n",
        "    print(\"Error: Mismatched number of WAV files or empty lists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlIsJl7Ouqnc",
        "outputId": "949bb044-38c3-43da-9cd1-08fe826ebcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Mismatched number of WAV files or empty lists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한 폴더 내에서 elementwise wasserstein distance 측정"
      ],
      "metadata": {
        "id": "ytJtoDNIT7dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import itertools\n",
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "# 🔥 저장된 Wav2Vec2 Feature Map 불러오기\n",
        "feature_dir = \"feature_output_wav2vec2_lsgan\"\n",
        "feature_files = sorted(glob.glob(f\"{feature_dir}/feature_*.npy\"))\n",
        "\n",
        "features = []\n",
        "\n",
        "# 🔥 Feature Map 로드 (모든 feature를 리스트에 저장)\n",
        "for file_path in feature_files:\n",
        "    feature_map = np.load(file_path)  # (1024,) 형태\n",
        "    features.append(feature_map)\n",
        "\n",
        "# 🔥 NumPy 배열 변환 (150, 1024)\n",
        "features = np.array(features)\n",
        "num_samples = features.shape[0]\n",
        "\n",
        "print(f\"✅ Feature Shape: {features.shape}\")  # (150, 1024)\n",
        "\n",
        "# 🔥 Pairwise Wasserstein Distance 계산\n",
        "distances = []\n",
        "for i, j in itertools.combinations(range(num_samples), 2):\n",
        "    dist = wasserstein_distance(features[i], features[j])\n",
        "    distances.append(dist)\n",
        "\n",
        "# 🔥 평균 Wasserstein Distance 출력\n",
        "mean_distance = np.mean(distances)\n",
        "print(f\"✅ 평균 Wasserstein Distance: {mean_distance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MieNAs8T_wh",
        "outputId": "e86dd674-9265-4342-f82b-4f68e8522736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Feature Shape: (150, 1024)\n",
            "✅ 평균 Wasserstein Distance: 0.0048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo search 후 Wasserstein Distance 측정"
      ],
      "metadata": {
        "id": "jI59ZAL6uXoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dir = \"feature_output_wav2vec2_lsgan\"  # 저장된 Wav2Vec2 Feature Map이 있는 폴더\n",
        "feature_files = sorted(glob.glob(f\"{feature_dir}/feature_*.npy\"))\n",
        "\n",
        "features = [np.load(file_path) for file_path in feature_files]  # (150, 1024)\n",
        "\n",
        "# 🔥 Monte Carlo Search 설정\n",
        "num_samples = 1000  # Monte Carlo 시뮬레이션 횟수\n",
        "num_pairs = 50  # 한 번의 샘플링에서 비교할 pair 개수\n",
        "\n",
        "wasserstein_distances = []\n",
        "\n",
        "# 🔥 Monte Carlo Search 실행\n",
        "for _ in range(num_samples):\n",
        "    sampled_indices = np.random.choice(len(features), num_pairs * 2, replace=False)\n",
        "    sampled_features = [features[i] for i in sampled_indices]\n",
        "\n",
        "    # Pairwise Wasserstein Distance 계산\n",
        "    for i in range(0, len(sampled_features), 2):\n",
        "        dist = wasserstein_distance(sampled_features[i], sampled_features[i + 1])\n",
        "        wasserstein_distances.append(dist)\n",
        "\n",
        "# 🔥 Wasserstein Distance 평균 및 분산 계산\n",
        "mean_wasserstein_distance = np.mean(wasserstein_distances)\n",
        "variance_wasserstein_distance = np.var(wasserstein_distances)\n",
        "\n",
        "print(f\"✅ Monte Carlo Search 후 Wasserstein Distance 평균: {mean_wasserstein_distance:.6f}\")\n",
        "print(f\"✅ Monte Carlo Search 후 Wasserstein Distance 분산: {variance_wasserstein_distance:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxgZ8JXuuay2",
        "outputId": "74df7e10-b909-44a8-bc6f-c5adc6750000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Monte Carlo Search 후 Wasserstein Distance 평균: 0.004777\n",
            "✅ Monte Carlo Search 후 Wasserstein Distance 분산: 0.000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Variance 측정"
      ],
      "metadata": {
        "id": "6EN9KArxz1Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔥 Feature 데이터 불러오기\n",
        "feature_dir = \"feature_output_wav2vec2_lsgan\"  # 저장된 Wav2Vec2 Feature Map이 있는 폴더\n",
        "feature_files = sorted(glob.glob(f\"{feature_dir}/feature_*.npy\"))\n",
        "\n",
        "# 🔥 Feature 로드\n",
        "features = [np.load(file_path) for file_path in feature_files]  # (150, 1024)\n",
        "features = np.array(features)  # (150, 1024) 형태로 변환\n",
        "\n",
        "# 각 데이터 포인트의 1024차원 평균값 (150개의 샘플에 대한 평균 벡터)\n",
        "mean_feature = np.mean(features, axis=0)  # (1024,)\n",
        "\n",
        "# 각 데이터 포인트와 평균 벡터 사이의 거리(유클리드 거리) 계산\n",
        "distances = np.linalg.norm(features - mean_feature, axis=1)  # (150,)\n",
        "\n",
        "# 150개 데이터 포인트의 분산 계산\n",
        "overall_variance = np.var(distances)\n",
        "\n",
        "print(f\"✅ 150개 데이터 포인트 간의 분산: {overall_variance:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kg_P2Rmz6Uk",
        "outputId": "3af14be9-7924-4931-d09c-3974168d6a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 150개 데이터 포인트 간의 분산: 0.045797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator 마지막 레이어를 제외한 feature로 t-SNE 시각화"
      ],
      "metadata": {
        "id": "a8Q8PeDhiP84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "segment_size = 8192\n",
        "hop_size = 256\n",
        "frames_per_seg = segment_size // hop_size  # 32\n",
        "\n",
        "# 🔥 저장된 Feature Map 파일 불러오기\n",
        "feature_dir = \"feature_output_wav2vec2_rpgan\"\n",
        "feature_files = sorted(glob.glob(f\"{feature_dir}/feature_*.npy\"))\n",
        "\n",
        "features_mean = []\n",
        "\n",
        "# 🔥 Feature Map 로드 및 길이 맞추기 (MelDataset 방식)\n",
        "for file_path in feature_files:\n",
        "    feature_map = np.load(file_path)  # (1, 32, T)\n",
        "    feature_map = feature_map.squeeze(0)  # (32, T)로 변환\n",
        "\n",
        "    # Time Mean Pooling 적용 → (32,)\n",
        "    feature_mean = np.mean(feature_map, axis=1)\n",
        "\n",
        "    # 현재 Feature Map 길이\n",
        "    current_length = feature_mean.shape[-1]\n",
        "\n",
        "    # 🔥 MelDataset과 같은 길이 맞추기 방식 적용\n",
        "    if current_length > frames_per_seg:\n",
        "        # 랜덤 위치에서 segment 크기만큼 샘플링\n",
        "        start = np.random.randint(0, current_length - frames_per_seg + 1)\n",
        "        feature_mean = feature_mean[start:start + frames_per_seg]\n",
        "\n",
        "    elif current_length < frames_per_seg:\n",
        "        # Zero Padding 적용\n",
        "        feature_mean = np.pad(feature_mean, (0, frames_per_seg - current_length), mode='constant')\n",
        "\n",
        "    features_mean.append(feature_mean)\n",
        "\n",
        "# 🔥 NumPy 배열 변환 (이제 길이가 동일함)\n",
        "features_mean = np.array(features_mean)\n",
        "\n",
        "# 🔥 t-SNE 적용 (32 → 2차원)\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "features_tsne = tsne.fit_transform(features_mean)\n",
        "\n",
        "\n",
        "# 🔥 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(features_tsne[:, 0], features_tsne[:, 1], alpha=0.7)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"t-SNE Visualization of Generator Feature Space\")\n",
        "plt.show(block=True)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.draw()\n",
        "plt.pause(1)  # 강제 업데이트"
      ],
      "metadata": {
        "id": "Bv0WRIzSVvBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wav2vec2 Feature 로 t-SNE"
      ],
      "metadata": {
        "id": "1KiMKtAKjWYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 🔥 저장된 Wav2Vec2 Feature Map 불러오기\n",
        "feature_dir = \"feature_output_wav2vec2_rpgan\"\n",
        "feature_files = sorted(glob.glob(f\"{feature_dir}/feature_*.npy\"))\n",
        "\n",
        "features = []\n",
        "\n",
        "# 🔥 Feature Map 로드 (1024,)\n",
        "for file_path in feature_files:\n",
        "    feature_map = np.load(file_path)  # (1024,)\n",
        "    features.append(feature_map)\n",
        "\n",
        "# 🔥 NumPy 배열 변환 (이미 모든 feature가 동일한 차원)\n",
        "features = np.array(features)  # (150, 1024)\n",
        "\n",
        "print(f\"✅ Feature Shape Before PCA: {features.shape}\")  # (150, 1024)\n",
        "\n",
        "# 🔥 PCA 적용 (1024 → 50차원)\n",
        "pca = PCA(n_components=50)\n",
        "features_pca = pca.fit_transform(features)\n",
        "\n",
        "print(f\"✅ Feature Shape After PCA: {features_pca.shape}\")  # (150, 50)\n",
        "\n",
        "# 🔥 t-SNE 적용 (50 → 2차원)\n",
        "tsne = TSNE(n_components=2, perplexity=15, random_state=42)\n",
        "features_tsne = tsne.fit_transform(features_pca)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-10, 10))\n",
        "features_tsne = scaler.fit_transform(features_tsne)\n",
        "\n",
        "min_x, max_x = -10, 10\n",
        "min_y, max_y = -10, 10\n",
        "# 🔥 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(features_tsne[:, 0], features_tsne[:, 1], alpha=0.7)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"t-SNE Visualization of Generator Feature Space\")\n",
        "plt.xlim(min_x, max_x)  # ✅ x축 범위 고정\n",
        "plt.ylim(min_y, max_y)  # ✅ y축 범위 고정\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "S9DotFXy9feK",
        "outputId": "d9bca8e0-f006-4558-8521-23f612623736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Feature Shape Before PCA: (150, 1024)\n",
            "✅ Feature Shape After PCA: (150, 50)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "do7m_5tORSW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 하나의 Mel을 반복적으로 inference\n"
      ],
      "metadata": {
        "id": "RGNHMpVtxEDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "class Args:\n",
        "    group_name = None\n",
        "    input_wavs_dir = 'LJSpeech-1.1/wavs'\n",
        "    output_dirs = 'inference_output_rpgan_0303'\n",
        "    checkpoint_file = 'cp_RPGAN_R1R2_lambda(0.0005)/g_00081810.pt'\n",
        "    input_validation_file = 'LJSpeech-1.1/validation.txt'\n",
        "    feature_output_dir = 'feature_output_rpgan'\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(f\"Loading checkpoint from '{filepath}'\")\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Checkpoint loaded.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(x, h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size, h.fmin, h.fmax)\n",
        "\n",
        "def get_inference_dataset(a, num_samples=1, start_idx=0):\n",
        "    \"\"\" 하나의 Mel Spectrogram만 가져오기 \"\"\"\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    return validation_files[start_idx:num_samples+start_idx]  # 한 개의 파일만 선택\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = get_inference_dataset(a, num_samples=1)  # 1개의 Mel Spectrogram만 가져오기\n",
        "\n",
        "    os.makedirs(a.output_dirs, exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        file_path = filelist[0]  # 하나의 Mel Spectrogram만 사용\n",
        "\n",
        "        # Mel Spectrogram 변환\n",
        "        wav, sr = load_wav(file_path)\n",
        "        wav = wav / MAX_WAV_VALUE\n",
        "        wav = torch.FloatTensor(wav).to(device)\n",
        "        x = get_mel(wav.unsqueeze(0))\n",
        "\n",
        "        # 같은 Mel Spectrogram으로 100번 생성\n",
        "        for i in range(1, 101):  # 1~100\n",
        "            y_g_hat, _ = generator(x)\n",
        "\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            # 파일명 변경 (각 반복마다 다르게 저장)\n",
        "            output_filename = os.path.splitext(os.path.basename(file_path))[0] + f'_rpgan_inference_{i:03d}.wav'\n",
        "            output_file = os.path.join(a.output_dirs, output_filename)\n",
        "\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(f\"Generated: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    config_file = \"config_v1.json\"\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a = Args()\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NstaYdPQxMq8",
        "outputId": "72dc560c-6de2-458b-f8e1-a063d4bd2357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Inference Process..\n",
            "Loading checkpoint from 'cp_RPGAN_R1R2_lambda(0.0005)/g_00081810.pt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-f76c4eeb10b0>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(filepath, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded.\n",
            "Removing weight norm...\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_001.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_002.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_003.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_004.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_005.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_006.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_007.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_008.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_009.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_010.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_011.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_012.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_013.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_014.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_015.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_016.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_017.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_018.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_019.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_020.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_021.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_022.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_023.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_024.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_025.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_026.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_027.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_028.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_029.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_030.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_031.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_032.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_033.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_034.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_035.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_036.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_037.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_038.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_039.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_040.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_041.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_042.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_043.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_044.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_045.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_046.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_047.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_048.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_049.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_050.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_051.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_052.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_053.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_054.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_055.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_056.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_057.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_058.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_059.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_060.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_061.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_062.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_063.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_064.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_065.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_066.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_067.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_068.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_069.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_070.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_071.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_072.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_073.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_074.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_075.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_076.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_077.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_078.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_079.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_080.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_081.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_082.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_083.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_084.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_085.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_086.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_087.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_088.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_089.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_090.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_091.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_092.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_093.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_094.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_095.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_096.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_097.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_098.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_099.wav\n",
            "Generated: inference_output_rpgan_0303/LJ050-0269_rpgan_inference_100.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIdebGGwxg8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}